{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "sns.set_palette('colorblind')\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report,confusion_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    df = pd.read_csv('newtrain.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_test_data():\n",
    "    df = pd.read_csv('newtest.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "\n",
    "    #tokenize text\n",
    "    pattern = r'''(?x)    # set flag to allow verbose regexps\n",
    "         ([A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "       | \\w+([-']\\w+)*        # words with optional internal hyphens\n",
    "       | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "       | [!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+  # these are separate tokens (string.punctuation)\n",
    "     '''\n",
    "    tokenize = lambda text: nltk.regexp_tokenize(text, pattern)\n",
    "    df['Tokens'] = df['Text'].apply(tokenize)\n",
    "    \n",
    "    def pos_tag(text):\n",
    "        tuples = nltk.pos_tag(text)\n",
    "        tags = []\n",
    "        for t in tuples:\n",
    "            tags.append(t[1])\n",
    "        return tags\n",
    "    def get_nouns(text):\n",
    "        wnlemmatizer = nltk.WordNetLemmatizer()\n",
    "        tuples = nltk.pos_tag(text)\n",
    "        tags = []\n",
    "        for t in tuples:\n",
    "            if t[1][0] == 'N':\n",
    "                tags.append(wnlemmatizer.lemmatize(t[0]))\n",
    "        return tags\n",
    "    df['Nouns'] = df['Tokens'].apply(get_nouns)\n",
    "    df['POS'] = df['Tokens'].apply(pos_tag)\n",
    "    make_string = lambda a: ' '.join(i for i in a)\n",
    "    df['Nouns'] = df['Nouns'].apply(make_string)\n",
    "    df['POS'] = df['POS'].apply(make_string)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vectorize data\n",
    "#turns words into a list of vectors - vector length is the total number of words\n",
    "#Vector elements correspond to 1 word (1/0 if word is/not present in the current item)\n",
    "def featurize(df, test_df, max_f, targets=True):\n",
    "    count_vec = CountVectorizer(analyzer = 'char_wb', ngram_range= (5,5), stop_words = ENGLISH_STOP_WORDS)\n",
    "    noun_vec = CountVectorizer(analyzer = 'char_wb')\n",
    "    pos_vec = CountVectorizer()\n",
    "    vec_tar = LabelEncoder()\n",
    "    targets = df['Category']\n",
    "        \n",
    "    counts = count_vec.fit_transform(df['Text'])\n",
    "    #print(\"Counts shape: \" + str(counts.shape))\n",
    "    nouns = noun_vec.fit_transform(df['Nouns'])\n",
    "    #print(\"Nouns shape: \" + str(nouns.shape))\n",
    "    pos = pos_vec.fit_transform(df['POS'])\n",
    "    #print(\"POS shape: \" + str(pos.shape))\n",
    "    \n",
    "    \n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    tfidf = tfidf_transformer.fit_transform(counts)\n",
    "    \n",
    "    \n",
    "    test = count_vec.transform(test_df['Text'])\n",
    "    test = tfidf_transformer.transform(test)\n",
    "    test_nouns = noun_vec.transform(test_df['Nouns'])\n",
    "    test_pos = pos_vec.transform(test_df['POS'])\n",
    "    \n",
    "    #To combine or not to combine\n",
    "    nouns_array = nouns.toarray()\n",
    "    pos_array = pos.toarray()\n",
    "    combined = np.hstack((nouns_array, pos_array))\n",
    "    \n",
    "    tnouns_array = test_nouns.toarray()\n",
    "    tpos_array = test_pos.toarray()\n",
    "    \n",
    "    tcombined = np.hstack((tnouns_array, tpos_array))\n",
    "    \n",
    "    return targets, tfidf, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just for the test data\n",
    "def get_test_features(df, count_vec, transformer):\n",
    "    \n",
    "    test_counts = count_vec.transform(df['Text'])\n",
    "    test_tfidf = transformer.tranform(test_counts)\n",
    "    return test_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#evaluate using 10 fold cross-validation, inspect results\n",
    "#KNN WITH CV#\n",
    "def cross_validate(targets, df):\n",
    "    #SVC\n",
    "    mod = LinearSVC(C=.1)\n",
    "    cv = StratifiedShuffleSplit(targets, n_iter=10, test_size=.1)\n",
    "    \n",
    "    scores = []\n",
    "    for tr, tt in cv:\n",
    "        train = df.loc[tr]\n",
    "        test = df.loc[tt]\n",
    "        result = featurize(train, test, 1000)\n",
    "        xtrain = result[1]\n",
    "        xtest = result[2]\n",
    "        mod.fit(xtrain, targets[tr])\n",
    "        scores.append(mod.score(xtest, targets[tt]))\n",
    "\n",
    "    print('\\nLinear SVC\\n\\t mean score: {0}'.format(np.mean(scores)))\n",
    "    \n",
    "    #svm\n",
    "    #mod = SGDClassifier(loss = 'hinge',penalty = 'L2',alpha = 1e-3)\n",
    "    #scores = []\n",
    "    #for tr, tt in cv:\n",
    "    #    mod.fit(features[tr], targets[tr])\n",
    "    #    scores.append(mod.score(features[tt],targets[tt]))\n",
    "    #print('\\nLinear SVM\\n\\t mean score: {0}'.format(np.mean(scores)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #naive bayes\n",
    "    mod = MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "    scores = []\n",
    "    for tr, tt in cv:\n",
    "        train = df.loc[tr]\n",
    "        test = df.loc[tt]\n",
    "        result = featurize(train, test, 1000)\n",
    "        xtrain = result[1]\n",
    "        xtest = result[2]\n",
    "        mod.fit(xtrain, targets[tr])\n",
    "        scores.append(mod.score(xtest, targets[tt]))\n",
    "    print('\\nNaive Bayes\\n\\t mean score: {0}'.format(np.mean(scores)))\n",
    "    \n",
    "    \n",
    "    logreg = LogisticRegression()\n",
    "    scores = []\n",
    "    for tr, tt in cv:\n",
    "        train = df.loc[tr]\n",
    "        test = df.loc[tt]\n",
    "        result = featurize(train, test, 1000)\n",
    "        xtrain = result[1]\n",
    "        xtest = result[2]\n",
    "        mod.fit(xtrain, targets[tr])\n",
    "        scores.append(mod.score(xtest, targets[tt]))\n",
    "    print('\\nLogReg\\n\\t mean score: {0}'.format(np.mean(scores)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title, target_names, cmap=plt.cm.coolwarm):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(target_names))\n",
    "    plt.xticks(tick_marks, target_names, rotation=45)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def examine_classification(model):\n",
    "    result = get_labels_features(df, 1000)\n",
    "    y = result[0]\n",
    "    x = result[1]\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(x, labels, test_size=0.33, random_state=99)\n",
    "    \n",
    "    model.fit(xtrain, ytrain)\n",
    "    ypredicted = model.predict(xtest)\n",
    "    print(accuracy_score(ytest, ypredicted))\n",
    "    print()\n",
    "    print(pd.crosstab(ytest, ypredicted, \n",
    "            rownames=['True'], colnames=['Predicted'], \n",
    "            margins=True))\n",
    "    print()\n",
    "    cm = confusion_matrix(ytest, ypredicted)\n",
    "    plot_confusion_matrix(cm, \"Confusion Matrix\", range(1,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Nouns</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>why are yawns contagious? when people yawn</td>\n",
       "      <td>[why, are, yawns, contagious, when, people, yawn]</td>\n",
       "      <td>yawn people</td>\n",
       "      <td>WRB VBP NNS JJ WRB NNS VBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>what is trans fat? how to reduce that? i heard...</td>\n",
       "      <td>[what, is, trans, fat, how, to, reduce, that, ...</td>\n",
       "      <td>trans tras body food</td>\n",
       "      <td>WP VBZ NNS JJ WRB TO VB IN PRP VBP IN NNS VBP ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>roth ira vs 401k? what is the difference betwe...</td>\n",
       "      <td>[roth, ira, vs, 401k, what, is, the, differenc...</td>\n",
       "      <td>roth ira v difference roth ira prefer</td>\n",
       "      <td>NN NN NNS CD WP VBZ DT NN IN NN NN CC CD WRB M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>how many planes fedex has? i heard that it is ...</td>\n",
       "      <td>[how, many, planes, fedex, has, i, heard, that...</td>\n",
       "      <td>plane fedex airline world</td>\n",
       "      <td>WRB JJ NNS NN VBZ PRP VBP IN PRP VBZ DT JJS NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>what is the best photo slideshow creation appl...</td>\n",
       "      <td>[what, is, the, best, photo, slideshow, creati...</td>\n",
       "      <td>photo slideshow creation application photo sli...</td>\n",
       "      <td>WP VBZ DT JJS NN NN NN NN WP VBZ DT JJS NN NN ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category                                               Text  \\\n",
       "0         5         why are yawns contagious? when people yawn   \n",
       "1         6  what is trans fat? how to reduce that? i heard...   \n",
       "2         1  roth ira vs 401k? what is the difference betwe...   \n",
       "3         1  how many planes fedex has? i heard that it is ...   \n",
       "4         2  what is the best photo slideshow creation appl...   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0  [why, are, yawns, contagious, when, people, yawn]   \n",
       "1  [what, is, trans, fat, how, to, reduce, that, ...   \n",
       "2  [roth, ira, vs, 401k, what, is, the, differenc...   \n",
       "3  [how, many, planes, fedex, has, i, heard, that...   \n",
       "4  [what, is, the, best, photo, slideshow, creati...   \n",
       "\n",
       "                                               Nouns  \\\n",
       "0                                        yawn people   \n",
       "1                               trans tras body food   \n",
       "2              roth ira v difference roth ira prefer   \n",
       "3                          plane fedex airline world   \n",
       "4  photo slideshow creation application photo sli...   \n",
       "\n",
       "                                                 POS  \n",
       "0                         WRB VBP NNS JJ WRB NNS VBP  \n",
       "1  WP VBZ NNS JJ WRB TO VB IN PRP VBP IN NNS VBP ...  \n",
       "2  NN NN NNS CD WP VBZ DT NN IN NN NN CC CD WRB M...  \n",
       "3  WRB JJ NNS NN VBZ PRP VBP IN PRP VBZ DT JJS NN...  \n",
       "4  WP VBZ DT JJS NN NN NN NN WP VBZ DT JJS NN NN ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_data()\n",
    "df = add_features(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df = get_test_data()\n",
    "test_df = add_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear SVC\n",
      "\t mean score: 0.5485185185185185\n",
      "\n",
      "Naive Bayes\n",
      "\t mean score: 0.42259259259259263\n",
      "\n",
      "LogReg\n",
      "\t mean score: 0.4177777777777778\n"
     ]
    }
   ],
   "source": [
    "#examine_classification(SGDClassifier(loss = 'hinge',penalty = 'L2',alpha = 1e-3))\n",
    "cross_validate(df['Category'], df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2698, 23364)\n",
      "(1874, 23364)\n"
     ]
    }
   ],
   "source": [
    "# Predicting on actual test data\n",
    "result = featurize(df, test_df, 1000)\n",
    "labels = result[0]\n",
    "x = result[1]\n",
    "print(x.shape)\n",
    "test = result[2]\n",
    "print (test.shape)\n",
    "#cross_validate(labels, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#result = get_test_features(test_df, count_vec, tfidf_trans)\n",
    "#test = result[1]\n",
    "#print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mod = LogisticRegression()\n",
    "mod.fit(x, labels)\n",
    "predictions = mod.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('submission.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['ID', 'Category'])\n",
    "    for i, p in enumerate(predictions):\n",
    "        writer.writerow([i + 1, p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
