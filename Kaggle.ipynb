{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best submission so far is ensemble with LinearSVC (not SVM), no max features, no KNN. (KNN hasn't been grid searched yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "sns.set_palette('colorblind')\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report,confusion_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    df = pd.read_csv('newtrain.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_test_data():\n",
    "    df = pd.read_csv('newtest.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    #tokenize text\n",
    "    pattern = r'''(?x)    # set flag to allow verbose regexps\n",
    "         ([A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "       | \\w+([-']\\w+)*        # words with optional internal hyphens\n",
    "       | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "       | [!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+  # these are separate tokens (string.punctuation)\n",
    "     '''\n",
    "    tokenize = lambda text: nltk.regexp_tokenize(text, pattern)\n",
    "    df['Tokens'] = df['Text'].apply(tokenize)\n",
    "    \n",
    "    def pos_tag(text):\n",
    "        tuples = nltk.pos_tag(text)\n",
    "        tags = []\n",
    "        for t in tuples:\n",
    "            tags.append(t[1])\n",
    "        return tags\n",
    "    \n",
    "    def get_nouns(text):\n",
    "        wnlemmatizer = nltk.WordNetLemmatizer()\n",
    "        tuples = nltk.pos_tag(text)\n",
    "        tags = []\n",
    "        for t in tuples:\n",
    "            if t[1][0] == 'N':\n",
    "                tags.append(wnlemmatizer.lemmatize(t[0]))\n",
    "        return tags\n",
    "    df['Nouns'] = df['Tokens'].apply(get_nouns)\n",
    "    df['POS'] = df['Tokens'].apply(pos_tag)\n",
    "    make_string = lambda a: ' '.join(i for i in a)\n",
    "    df['Nouns'] = df['Nouns'].apply(make_string)\n",
    "    df['POS'] = df['POS'].apply(make_string)\n",
    "    \n",
    "    df['TokenString'] = df['Tokens'].apply(make_string)\n",
    "    \n",
    "    df['TokensNouns'] = df[['TokenString', 'Nouns']].apply(lambda x: ' '.join(x), axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#vectorize data\n",
    "#turns words into a list of vectors - vector length is the total number of words\n",
    "#Vector elements correspond to 1 word (1/0 if word is/not present in the current item)\n",
    "def featurize(df, test_df, count_vec):\n",
    "    pos_vec = CountVectorizer()\n",
    "    vec_tar = LabelEncoder()\n",
    "    targets = df['Category']\n",
    "        \n",
    "    counts = count_vec.fit_transform(df['TokensNouns'])\n",
    "    nouns = noun_vec.fit_transform(df['Nouns'])\n",
    "    pos = pos_vec.fit_transform(df['POS'])\n",
    "    \n",
    "    \n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    tfidf = tfidf_transformer.fit_transform(counts)\n",
    "    \n",
    "    \n",
    "    test = count_vec.transform(test_df['TokensNouns'])\n",
    "    test = tfidf_transformer.transform(test)\n",
    "    test_nouns = noun_vec.transform(test_df['Nouns'])\n",
    "    test_pos = pos_vec.transform(test_df['POS'])\n",
    "    \n",
    "    return tfidf, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Takes in training data, training labels, predicts labels for xtest using three methods.\n",
    "def ensemble_predictor(df, test_df): \n",
    "    mod1 = LinearSVC(C=1) #vec analyzer = word, ngram(1,1)\n",
    "    mod2 = MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "    mod3 = LogisticRegression(C=1000)#vec analyzer = word, ngram(1,3)\n",
    "    mod4 = KNeighborsClassifier(n_neighbors=10)\n",
    "    mod5 = SVC(C=1000, gamma=.01)\n",
    "    \n",
    "    ytrain = df['Category']\n",
    "    xtrain1, xtest1 = featurize(df, test_df, CountVectorizer(analyzer='word', ngram_range=(1,1)))\n",
    "    #xtrain2, xtest2 = featurize(df, test_df, CountVectorizer(analyzer='word', ngram_range=(1,1)))\n",
    "    xtrain3, xtest3 = featurize(df, test_df, CountVectorizer(analyzer='word', ngram_range=(1,3)))\n",
    "    xtrain4, xtest4 = featurize(df, test_df, CountVectorizer(analyzer='char_wb', ngram_range=(5, 5)))\n",
    "    xtrain5, xtest5 = featurize(df, test_df, CountVectorizer(analyzer='word', ngram_range=(1, 3)))\n",
    "    \n",
    "    \n",
    "    mod1.fit(xtrain1, ytrain)\n",
    "    #mod2.fit(xtrain2, ytrain)\n",
    "    mod3.fit(xtrain3, ytrain)\n",
    "    mod4.fit(xtrain4, ytrain)\n",
    "    mod5.fit(xtrain5, ytrain)\n",
    "    \n",
    "    y1 = mod1.predict(xtest1)\n",
    "    #y2 = mod2.predict(xtest2)\n",
    "    y3 = mod3.predict(xtest3)\n",
    "    y4 = mod4.predict(xtest4)\n",
    "    y5 = mod5.predict(xtest5)\n",
    "    \n",
    "    #votes = zip(y1, y2, y3, y4, y5)\n",
    "    votes = zip(y1,y3,y4,y5)\n",
    "    ypredicted = []\n",
    "    count = 0\n",
    "    for v in votes:\n",
    "        v = list(v)\n",
    "        #shuffle(v)\n",
    "        p = max(set(v), key=v.count)\n",
    "        if len(np.unique(v)) == len(v):\n",
    "            count += 1\n",
    "            p = v[1]\n",
    "        ypredicted.append(p)\n",
    "    print(\"count: \" + str(count))\n",
    "    print(\"total number of predictions: \" + str(len(ypredicted)))\n",
    "    return ypredicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#evaluate using 10 fold cross-validation, inspect results\n",
    "#KNN WITH CV#\n",
    "def cross_validate(targets, df):\n",
    "    \n",
    "    #LinearSVC\n",
    "    mod = LinearSVC(C=1)\n",
    "    cv = StratifiedShuffleSplit(targets, n_iter=10, test_size=.1)\n",
    "    \n",
    "    scores = []\n",
    "    for tr, tt in cv:\n",
    "        train = df.loc[tr]\n",
    "        test = df.loc[tt]\n",
    "        result = featurize(train, test, CountVectorizer(analyzer='word', ngram_range=(1,1)))\n",
    "        xtrain = result[0]\n",
    "        xtest = result[1]\n",
    "        mod.fit(xtrain, targets[tr])\n",
    "        scores.append(mod.score(xtest, targets[tt]))\n",
    "\n",
    "    print('\\nLinear SVC\\n\\t mean score: {0}'.format(np.mean(scores)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #SVC\n",
    "    mod = SVC(C=1000, gamma=0.01)\n",
    "    cv = StratifiedShuffleSplit(targets, n_iter=10, test_size=.1)\n",
    "    \n",
    "    scores = []\n",
    "    for tr, tt in cv:\n",
    "        train = df.loc[tr]\n",
    "        test = df.loc[tt]\n",
    "        result = featurize(train, test, CountVectorizer(analyzer='word', ngram_range=(1, 3)))\n",
    "        xtrain = result[0]\n",
    "        xtest = result[1]\n",
    "        mod.fit(xtrain, targets[tr])\n",
    "        scores.append(mod.score(xtest, targets[tt]))\n",
    "\n",
    "    print('\\nRBF SVC\\n\\t mean score: {0}'.format(np.mean(scores)))\n",
    "    \n",
    "    \n",
    "    #naive bayes\n",
    "    mod = MultinomialNB(alpha=1.0, fit_prior=True)\n",
    "    scores = []\n",
    "    for tr, tt in cv:\n",
    "        train = df.loc[tr]\n",
    "        test = df.loc[tt]\n",
    "        result = featurize(train, test, CountVectorizer(analyzer='word', ngram_range=(1,1)))\n",
    "        xtrain = result[0]\n",
    "        xtest = result[1]\n",
    "        mod.fit(xtrain, targets[tr])\n",
    "        scores.append(mod.score(xtest, targets[tt]))\n",
    "    print('\\nNaive Bayes\\n\\t mean score: {0}'.format(np.mean(scores)))\n",
    "    \n",
    "    \n",
    "    mod = LogisticRegression(C=1000)\n",
    "    scores = []\n",
    "    for tr, tt in cv:\n",
    "        train = df.loc[tr]\n",
    "        test = df.loc[tt]\n",
    "        result = featurize(train, test, CountVectorizer(analyzer='word', ngram_range=(1,3)))\n",
    "        xtrain = result[0]\n",
    "        xtest = result[1]\n",
    "        mod.fit(xtrain, targets[tr])\n",
    "        scores.append(mod.score(xtest, targets[tt]))\n",
    "    print('\\nLogReg\\n\\t mean score: {0}'.format(np.mean(scores)))\n",
    "    \n",
    "    \n",
    "    mod = KNeighborsClassifier(n_neighbors=10)\n",
    "    scores = []\n",
    "    for tr, tt in cv:\n",
    "        train = df.loc[tr]\n",
    "        test = df.loc[tt]\n",
    "        result = featurize(train, test, CountVectorizer(analyzer='char_wb', ngram_range=(5, 5)))\n",
    "        xtrain = result[0]\n",
    "        xtest = result[1]\n",
    "        mod.fit(xtrain, targets[tr])\n",
    "        scores.append(mod.score(xtest, targets[tt]))\n",
    "    print('\\nKNN\\n\\t mean score: {0}'.format(np.mean(scores)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    scores = []\n",
    "    for tr, tt in cv:\n",
    "        train = df.loc[tr]\n",
    "        test = df.loc[tt]\n",
    "        predictions = ensemble_predictor(train, test)\n",
    "        scores.append(accuracy_score(targets[tt], predictions))\n",
    "    print('\\nEnsemble Predictor\\n\\t mean score: {0}'.format(np.mean(scores)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title, target_names, cmap=plt.cm.coolwarm):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(target_names))\n",
    "    plt.xticks(tick_marks, target_names, rotation=45)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def examine_classification(model):\n",
    "    result = get_labels_features(df, 1000)\n",
    "    y = result[0]\n",
    "    x = result[1]\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(x, labels, test_size=0.33, random_state=99)\n",
    "    \n",
    "    model.fit(xtrain, ytrain)\n",
    "    ypredicted = model.predict(xtest)\n",
    "    print(accuracy_score(ytest, ypredicted))\n",
    "    print()\n",
    "    print(pd.crosstab(ytest, ypredicted, \n",
    "            rownames=['True'], colnames=['Predicted'], \n",
    "            margins=True))\n",
    "    print()\n",
    "    #cm = confusion_matrix(ytest, ypredicted)\n",
    "    #plot_confusion_matrix(cm, \"Confusion Matrix\", range(1,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Nouns</th>\n",
       "      <th>POS</th>\n",
       "      <th>TokenString</th>\n",
       "      <th>TokensNouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>why are yawns contagious? when people yawn</td>\n",
       "      <td>[why, are, yawns, contagious, when, people, yawn]</td>\n",
       "      <td>yawn people</td>\n",
       "      <td>WRB VBP NNS JJ WRB NNS VBP</td>\n",
       "      <td>why are yawns contagious when people yawn</td>\n",
       "      <td>why are yawns contagious when people yawn yawn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>what is trans fat? how to reduce that? i heard...</td>\n",
       "      <td>[what, is, trans, fat, how, to, reduce, that, ...</td>\n",
       "      <td>trans tras body food</td>\n",
       "      <td>WP VBZ NNS JJ WRB TO VB IN PRP VBP IN NNS VBP ...</td>\n",
       "      <td>what is trans fat how to reduce that i heard t...</td>\n",
       "      <td>what is trans fat how to reduce that i heard t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>roth ira vs 401k? what is the difference betwe...</td>\n",
       "      <td>[roth, ira, vs, 401k, what, is, the, differenc...</td>\n",
       "      <td>roth ira v difference roth ira prefer</td>\n",
       "      <td>NN NN NNS CD WP VBZ DT NN IN NN NN CC CD WRB M...</td>\n",
       "      <td>roth ira vs 401k what is the difference betwee...</td>\n",
       "      <td>roth ira vs 401k what is the difference betwee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>how many planes fedex has? i heard that it is ...</td>\n",
       "      <td>[how, many, planes, fedex, has, i, heard, that...</td>\n",
       "      <td>plane fedex airline world</td>\n",
       "      <td>WRB JJ NNS NN VBZ PRP VBP IN PRP VBZ DT JJS NN...</td>\n",
       "      <td>how many planes fedex has i heard that it is t...</td>\n",
       "      <td>how many planes fedex has i heard that it is t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>what is the best photo slideshow creation appl...</td>\n",
       "      <td>[what, is, the, best, photo, slideshow, creati...</td>\n",
       "      <td>photo slideshow creation application photo sli...</td>\n",
       "      <td>WP VBZ DT JJS NN NN NN NN WP VBZ DT JJS NN NN ...</td>\n",
       "      <td>what is the best photo slideshow creation appl...</td>\n",
       "      <td>what is the best photo slideshow creation appl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category                                               Text  \\\n",
       "0         5         why are yawns contagious? when people yawn   \n",
       "1         6  what is trans fat? how to reduce that? i heard...   \n",
       "2         1  roth ira vs 401k? what is the difference betwe...   \n",
       "3         1  how many planes fedex has? i heard that it is ...   \n",
       "4         2  what is the best photo slideshow creation appl...   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0  [why, are, yawns, contagious, when, people, yawn]   \n",
       "1  [what, is, trans, fat, how, to, reduce, that, ...   \n",
       "2  [roth, ira, vs, 401k, what, is, the, differenc...   \n",
       "3  [how, many, planes, fedex, has, i, heard, that...   \n",
       "4  [what, is, the, best, photo, slideshow, creati...   \n",
       "\n",
       "                                               Nouns  \\\n",
       "0                                        yawn people   \n",
       "1                               trans tras body food   \n",
       "2              roth ira v difference roth ira prefer   \n",
       "3                          plane fedex airline world   \n",
       "4  photo slideshow creation application photo sli...   \n",
       "\n",
       "                                                 POS  \\\n",
       "0                         WRB VBP NNS JJ WRB NNS VBP   \n",
       "1  WP VBZ NNS JJ WRB TO VB IN PRP VBP IN NNS VBP ...   \n",
       "2  NN NN NNS CD WP VBZ DT NN IN NN NN CC CD WRB M...   \n",
       "3  WRB JJ NNS NN VBZ PRP VBP IN PRP VBZ DT JJS NN...   \n",
       "4  WP VBZ DT JJS NN NN NN NN WP VBZ DT JJS NN NN ...   \n",
       "\n",
       "                                         TokenString  \\\n",
       "0          why are yawns contagious when people yawn   \n",
       "1  what is trans fat how to reduce that i heard t...   \n",
       "2  roth ira vs 401k what is the difference betwee...   \n",
       "3  how many planes fedex has i heard that it is t...   \n",
       "4  what is the best photo slideshow creation appl...   \n",
       "\n",
       "                                         TokensNouns  \n",
       "0  why are yawns contagious when people yawn yawn...  \n",
       "1  what is trans fat how to reduce that i heard t...  \n",
       "2  roth ira vs 401k what is the difference betwee...  \n",
       "3  how many planes fedex has i heard that it is t...  \n",
       "4  what is the best photo slideshow creation appl...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_data()\n",
    "df = add_features(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear SVC\n",
      "\t mean score: 0.5914814814814815\n",
      "\n",
      "RBF SVC\n",
      "\t mean score: 0.5722222222222222\n",
      "\n",
      "Naive Bayes\n",
      "\t mean score: 0.4159259259259259\n",
      "\n",
      "LogReg\n",
      "\t mean score: 0.552962962962963\n",
      "\n",
      "KNN\n",
      "\t mean score: 0.5296296296296296\n",
      "count: 3\n",
      "total number of predictions: 270\n",
      "count: 0\n",
      "total number of predictions: 270\n",
      "count: 2\n",
      "total number of predictions: 270\n",
      "count: 3\n",
      "total number of predictions: 270\n",
      "count: 1\n",
      "total number of predictions: 270\n",
      "count: 2\n",
      "total number of predictions: 270\n",
      "count: 1\n",
      "total number of predictions: 270\n",
      "count: 1\n",
      "total number of predictions: 270\n",
      "count: 2\n",
      "total number of predictions: 270\n",
      "count: 1\n",
      "total number of predictions: 270\n",
      "\n",
      "Ensemble Predictor\n",
      "\t mean score: 0.5844444444444444\n"
     ]
    }
   ],
   "source": [
    "cross_validate(df['Category'], df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:   23.5s\n",
      "[Parallel(n_jobs=-1)]: Done  84 out of  90 | elapsed:   35.8s remaining:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:   36.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc:\n",
      "\tbest score: 0.5729128014842301\n",
      "parameters: {'vect__ngram_range': (1, 1), 'vect__analyzer': 'word', 'clf__C': 1}\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 200 jobs       | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 264 out of 270 | elapsed:  5.5min remaining:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done 270 out of 270 | elapsed:  5.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm:\n",
      "\tbest score: 0.5692022263450834\n",
      "parameters: {'vect__ngram_range': (1, 3), 'clf__gamma': 0.01, 'vect__analyzer': 'word', 'clf__C': 1000.0}\n"
     ]
    }
   ],
   "source": [
    "pipeline, parameters = [dict() for i in range(2)]\n",
    "#set up cross validation folds\n",
    "cv = StratifiedShuffleSplit(df.Category, n_iter=5, test_size=.2)\n",
    "\n",
    "#analysis pipeline with linear svc\n",
    "pipeline['svc'] = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words = ENGLISH_STOP_WORDS)),\n",
    "    ('tfidf', TfidfTransformer(use_idf = True)),\n",
    "    ('clf', LinearSVC())])\n",
    "\n",
    "parameters['svc'] = {\n",
    "    'vect__ngram_range': ((1, 1), (1,3), (5, 5)),\n",
    "    'vect__analyzer' : ('char_wb','word'),\n",
    "    'clf__C': (1, 1e3, 1e-3)}\n",
    "\n",
    "#analysis pipeline with svm with rbf kernel\n",
    "pipeline['svm'] = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words = ENGLISH_STOP_WORDS)),\n",
    "    ('tfidf', TfidfTransformer(use_idf = True)),\n",
    "    ('clf', SVC())])\n",
    "\n",
    "parameters['svm'] = {\n",
    "    'vect__ngram_range': ((1, 1), (1,3), (5, 5)),\n",
    "    'vect__analyzer' : ('char_wb','word'),\n",
    "    'clf__C': (1, 1e3, 1e-3),\n",
    "    'clf__gamma': (1, 1e2, 1e-2)}\n",
    "\n",
    "#analysis pipeline with logistic regression\n",
    "pipeline['log'] = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words = ENGLISH_STOP_WORDS)),\n",
    "    ('tfidf', TfidfTransformer(use_idf = True)),\n",
    "    ('clf', LogisticRegression(penalty = 'l2', solver = 'lbfgs', multi_class = 'multinomial'))])\n",
    "        \n",
    "parameters['log'] = {\n",
    "    'vect__ngram_range': ((1, 1), (1,2), (1,3), (5, 5)),\n",
    "    'vect__analyzer' : ('char_wb','word'),\n",
    "    'clf__C': (1, 1e3, 1e-3)}\n",
    "\n",
    "#analysis pipeline with naive bayes\n",
    "pipeline['nb'] = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words = ENGLISH_STOP_WORDS)),\n",
    "    ('tfidf', TfidfTransformer(use_idf = True)),\n",
    "    ('clf', MultinomialNB(fit_prior = True))])\n",
    "parameters['nb'] = {\n",
    "    'vect__ngram_range': ((1, 1), (1,2), (1,3), (5, 5)),\n",
    "    'vect__analyzer' : ('char_wb','word')}\n",
    "\n",
    "#analysis pipeline with kmeans\n",
    "pipeline['knn'] = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words = ENGLISH_STOP_WORDS)),\n",
    "    ('tfidf', TfidfTransformer(use_idf = True)),\n",
    "    ('clf', KNeighborsClassifier())])\n",
    "parameters['knn'] = {\n",
    "    'vect__ngram_range': ((1, 1), (1,2), (1,3), (5, 5)),\n",
    "    'vect__analyzer' : ('char_wb','word'),\n",
    "    'clf__n_neighbors': (8, 9, 10, 15),\n",
    "    'clf__weights': ('uniform', 'distance')}\n",
    "\n",
    "\n",
    "#fit grid search instance\n",
    "for m in ['svc','svm']: #,'log','nb', 'knn']:\n",
    "#for m in ['knn']:\n",
    "    scorer = 'accuracy'\n",
    "    gs_clf = GridSearchCV(pipeline[m], parameters[m], verbose = 1, cv = cv, n_jobs = -1, scoring = scorer)\n",
    "\n",
    "    gs_clf = gs_clf.fit(df['TokensNouns'], df.Category)\n",
    "\n",
    "    best_parameters, score, _ = max(gs_clf.grid_scores_, key=lambda x: x[1]) #find best params\n",
    "\n",
    "    print ('{0}:\\n\\tbest score: {1}\\nparameters: {2}'.format(m, score, best_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df = get_test_data()\n",
    "test_df = add_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 12\n",
      "total number of predictions: 1874\n"
     ]
    }
   ],
   "source": [
    "predictions = ensemble_predictor(df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('submission8.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['ID', 'Category'])\n",
    "    for i, p in enumerate(predictions):\n",
    "        writer.writerow([i + 1, p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
